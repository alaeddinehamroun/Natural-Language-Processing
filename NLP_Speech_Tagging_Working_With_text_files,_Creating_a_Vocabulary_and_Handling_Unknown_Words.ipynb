{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Speech_Tagging_Working With text files, Creating a Vocabulary and Handling Unknown Words.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONer09nYdM2c9doPR1nQWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaeddinehamroun/Natural-Language-Processing/blob/main/NLP_Speech_Tagging_Working_With_text_files%2C_Creating_a_Vocabulary_and_Handling_Unknown_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ufu4-B3rjWcJ"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Text Data\n",
        "A tagged dataset taken from the Wall Street Journal is provided in the file WSJ_02-21.pos."
      ],
      "metadata": {
        "id": "Kd_N4e-lj-_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read lines from 'WSJ_02-21.pos' file and save them into th 'lines' variable\n",
        "with open(\"WSJ_02-21.pos\",  'r') as f:\n",
        "  lines = f.readlines()"
      ],
      "metadata": {
        "id": "SQOwl26ej-qk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print columns for reference\n",
        "print(\"\\t\\tWord\", \"\\tTag\\n\")\n",
        "\n",
        "# Print first five lines of the dataset\n",
        "for i in range(5):\n",
        "    print(f'line number {i+1}: {lines[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqDBfs-Nj1WL",
        "outputId": "d28245c0-10c3-46a7-8a48-ba33a2bd5a02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\tWord \tTag\n",
            "\n",
            "line number 1: In\tIN\n",
            "\n",
            "line number 2: an\tDT\n",
            "\n",
            "line number 3: Oct.\tNNP\n",
            "\n",
            "line number 4: 19\tCD\n",
            "\n",
            "line number 5: review\tNN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first line (unformatted)\n",
        "lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "w6W0CuKoj4jA",
        "outputId": "a784a9ec-c77c-43fe-a614-7666abc15731"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In\\tIN\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a vocabulary\n",
        "A vocabulary is made up of every word that appeared at least 2 times in the dataset."
      ],
      "metadata": {
        "id": "N5lvB-IHmGvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the words from each line in the dataset\n",
        "words = [line.split('\\t')[0] for line in lines]"
      ],
      "metadata": {
        "id": "W2kFsx4_j48y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define defaultdict of type 'int'\n",
        "freq = defaultdict(int)\n",
        "\n",
        "# Count frequency of ocurrence for each word in the dataset\n",
        "for word in words:\n",
        "    freq[word] += 1"
      ],
      "metadata": {
        "id": "sQYqutbOj6jO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dict to only include words that appeared at least 2 times\n",
        "# Create the vocabulary by filtering the 'freq' dictionary\n",
        "vocab = [k for k, v in freq.items() if (v > 1 and k != '\\n')]"
      ],
      "metadata": {
        "id": "nYTrb_tqnl80"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the vocabulary\n",
        "vocab.sort()\n",
        "\n",
        "# Print some random values of the vocabulary\n",
        "for i in range(4000, 4005):\n",
        "    print(vocab[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmuWAj_Anw5v",
        "outputId": "499bb153-caf3-4a74-b708-2860c3b48247"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early\n",
            "Earnings\n",
            "Earth\n",
            "Earthquake\n",
            "East\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing new text sources"
      ],
      "metadata": {
        "id": "MQy_O1cRn5nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with unknown words"
      ],
      "metadata": {
        "id": "rinRl9TIoCiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have a vocabulary, you will use it when processing new text sources. A new text will have words that do not appear in the current vocabulary. To tackle this, you can simply classify each new word as an unknown one, but you can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding unknown token.\n",
        "This function will do the following checks and return an appropriate token:\n",
        "\n",
        "* Check if the unknown word contains any character that is a digit: return --unk_digit--\n",
        "* Check if the unknown word contains any punctuation character: return --unk_punct--\n",
        "* Check it the unkown word contains any upper-case character: return --unk_upper--\n",
        "* Check if the unkown word ends with a suffix that could indicate it is a noun, verb, adjective, or adverb: return --unk_noun--, --unk_verb--, --unk_adj--, --unk_adv-- respectively\n",
        "\n",
        "\n",
        "If a word fails to fall under any condition then its token will be a plain --unk--. The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns."
      ],
      "metadata": {
        "id": "xTTYZDPpoR_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_unk(word):\n",
        "    \"\"\"\n",
        "    Assign tokens to unknown words\n",
        "    \"\"\"\n",
        "    \n",
        "    # Punctuation characters\n",
        "    # Try printing them out in a new cell!\n",
        "    punct = set(string.punctuation)\n",
        "    \n",
        "    # Suffixes\n",
        "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
        "\n",
        "    # Loop the characters in the word, check if any is a digit\n",
        "    if any(char.isdigit() for char in word):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Loop the characters in the word, check if any is a punctuation character\n",
        "    elif any(char in punct for char in word):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Loop the characters in the word, check if any is an upper case character\n",
        "    elif any(char.isupper() for char in word):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Check if word ends with any noun suffix\n",
        "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Check if word ends with any verb suffix\n",
        "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Check if word ends with any adjective suffix\n",
        "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Check if word ends with any adverb suffix\n",
        "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "    \n",
        "    # If none of the previous criteria is met, return plain unknown\n",
        "    return \"--unk--\""
      ],
      "metadata": {
        "id": "xi5LcF9EoE5e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A POS tagger will always encounter words that are not within the vocabulary that is being used. By augmenting the dataset to include these unknown word tokens you are helping the tagger to have a better idea of the appropriate tag for these words."
      ],
      "metadata": {
        "id": "d_9tvKgdpqbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the correct tag for a word"
      ],
      "metadata": {
        "id": "pMQ_JuiQpsTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_tag(line, vocab):\n",
        "    # If line is empty return placeholders for word and tag\n",
        "    if not line.split():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "    else:\n",
        "        # Split line to separate word and tag\n",
        "        word, tag = line.split()\n",
        "        # Check if word is not in vocabulary\n",
        "        if word not in vocab: \n",
        "            # Handle unknown word\n",
        "            word = assign_unk(word)\n",
        "    return word, tag"
      ],
      "metadata": {
        "id": "Yq_9UJANpqIg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_word_tag('\\n', vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6h0_6ZIqzmO",
        "outputId": "2bfef62d-9e27-4068-d2bf-5ea6041a3ebc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('--n--', '--s--')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_word_tag('In\\tIN\\n', vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CosjkJDlq3XX",
        "outputId": "747c245f-091f-4e73-a80d-f97d1e11ea4b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('In', 'IN')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_word_tag('tardigrade\\tNN\\n', vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfENohmXq9X3",
        "outputId": "5d574579-28a8-4d35-b717-28037a6b7fc9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('--unk--', 'NN')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_word_tag('scrutinize\\tVB\\n', vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNZks6KxrJz7",
        "outputId": "3c5d3105-2937-4c53-aef8-f4499ae340ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('--unk_verb--', 'VB')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}